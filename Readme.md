# Записки сумасшедшего
## Структура
- `main.py` - собсна сам код линейной регрессии.
- `utility_func.py` - функция нормализации для градиента, и фильтр
- `NN_classes.py` - классы используемые в реализации

## Шиза
>[!danger]
>Хули тут так мало написано, опять все посмотрел и забыл расписать

**Высокая корреляция** - признаки избыточны. Можно применить фильтрацию.

**Фильтр высокой корреляции** - по пороговому значения корреляция удаляем один из двух скорреляриванных признаков. 

*Плюсы*: ^8ff66f
 - Упрощение модели.
 - Уменьшение вычислительной сложности
 - Что то про умное слово **мультиколлинеарность**.
 
*Минусы*:
- Потеря информации.
- Влияние на нелинейные взаимосвязи признаков.
- Смещение выборки, фильтр с высокой корреляцией может привести к смещению выборки, если он удаляет признаки, важные для прогнозирования зависимой переменной.

**Прототип функции:** - `high_correlation_filter(data, corr_matrix,corr_threshold = 1)` 
Значение по базе - 1, означает отсутствие фильтрации. [[#^8ff66f | это к этому]]

`loss_value` - должен быть скаляром.**Сделать проверку!!**

**Identity** функция активации - линейная функция активации, никак не изменяет данные на выходе слоя (другие варианты изменяют, к примеру в ReLu отсеивают отрицательные значения).

**Forward pass**: NN(Linear + act) -> Loss(devi -> MSE)
**Backward pass**: Loss(MSE -> devi) -> NN(act -> Linear)

это  что ли единичный перцептрон получается, если я правильно осознаю

